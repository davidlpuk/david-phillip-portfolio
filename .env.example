# Ollama Configuration (local or cloud)
# For local Ollama, leave OLLAMA_BASE_URL empty or use http://localhost:11434
# For Ollama Cloud, set your API key
OLLAMA_API_KEY=your-ollama-api-key-here
OLLAMA_BASE_URL=http://localhost:11434

# Chat API Server Configuration
PORT=3001

# Embedding Model (Ollama model for embeddings)
# Default: nomic-embed-text
EMBEDDING_MODEL=nomic-embed-text

# LLM Model (Ollama model for chat)
# Default: llama3.2
LLM_MODEL=llama3.2

# Frontend Configuration
VITE_CHAT_API_URL=http://localhost:3001/api
